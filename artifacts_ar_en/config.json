{
  "encoder_embedding_size": 300,
  "decoder_embedding_size": 300,
  "hidden_size": 512,
  "num_layers": 4,
  "enc_dropout": 0.5,
  "dec_dropout": 0.5,
  "pad_idx": 0,
  "sos_idx": 1,
  "eos_idx": 2,
  "unk_idx": 3,
  "library": "pytorch",
  "format": "seq2seq_lstm_v1"
}